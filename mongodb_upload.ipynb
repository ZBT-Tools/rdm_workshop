{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research data management with Python and MongoDB\n",
    "\n",
    "This notebook details an example upload process from parsing raw data to pushing it into the MongoDB database.\n",
    "A second notebbok will follow this up with querying data and retrieving it to perform analyses on it.\n",
    "\n",
    "## Parsing files in Python\n",
    "\n",
    "Before we can push data to the database, we need to parse a datafile. EVN generously provided us with a sample dataset found in `data/BZ011_Rohdaten.dat`. Before trying top parse it we should take a look at the file ourselves to understand its structure. Doing that we should notice the following:\n",
    "- The file is structured as a csv file, but uses tabs as delimiters between values instead of commas\n",
    "- The first row of the file contains column headers as strings and all following rows contain mixed data\n",
    "- The data is a mix of strings, decimal numbers and dates\n",
    "- The decimal numbers use commas instead of decimal separators (as opposed to points, which are used in english speaking countries and therefore also in virtually all programming languages) \n",
    "- Some column headers contain special characters, i.e. `Â°C` \n",
    "\n",
    "We will take care of the last point first. Special characters often cause problems with text encoding if they are not handled consistently. Therefore we use the `chardet` module to automatically detect the encoding of our input file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "# File path\n",
    "file_path = \"./data/BZ011_Rohdaten.dat\"\n",
    "file_path_metadata = \"./data/metadata_BZ011_Rohdaten.json\"\n",
    "\n",
    "\n",
    "with open(file_path, \"rb\") as f:\n",
    "    result = chardet.detect(f.read(100000))  # Analyze first 100KB\n",
    "    detected_encoding = result[\"encoding\"]\n",
    "\n",
    "print(detected_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then parse the file using the function `pd.read_csv()`, which conveniently accepts arguments to adjust the decimal separator and delimiters. We also want to convert the `Datum` column into a proper date format using `pd.to_datatime()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into pandas DataFrame\n",
    "df = pd.read_csv(file_path, delimiter=\"\\t\", encoding=detected_encoding, decimal=\",\")  \n",
    "df['Datum'] = pd.to_datetime(df['Datum'], format=\"%d.%m.%y %H:%M:%S\")  \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we have done all the necessary parsing to get our input file into a pandas dataframe. Some additional parsing methods, like combining multiple files can be found in the Repository of the recent Python course: https://github.com/ZBT-Tools/Python_workshop , in the notebook of part 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to MongoDB\n",
    "\n",
    "To upload data to MongoDB we start by creating a connection using the `pymongo` module and particularly its `MongoClient` function. The ZBT database can be reached with the following connection string:\n",
    "`mongodb://username:password@172.16.134.8:27017/?directConnection=true&authSource=admin`, where username and password need to be replaced by your own credentials. For users that exist only on a specific database, e.g. student users, the `authSource` parameter needs to be set to that database.\n",
    "Naturally, we do not want our credentials to be plainly visible in a Python script - especially if we want to push it to a GitHub repository at some point. Any such secrets should be stored in environment variable files, which are conventionally called `.env` (but can be called however you prefer). \n",
    "\n",
    "```\n",
    "MONGODB_USER = \"username\"\n",
    "MONGODB_PASSWORD = \"password\"\n",
    "```\n",
    "\n",
    "Before committing your files to a git repository you should then create a `.gitignore` file, which contains the name of your environment file. For your testing purposes, I have added the `.env` file to this repository, so you can use the test account from the course with username `rdm_workshop` and password `password`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "mongodb_user = os.environ.get(\"MONGODB_USER\")\n",
    "mongodb_password = os.environ.get(\"MONGODB_PASS\")\n",
    "# MongoDB connection\n",
    "mongo_uri = \"mongodb://\"+mongodb_user+\":\"+mongodb_password+\"@172.16.134.8:27017/?directConnection=true&authSource=admin\"\n",
    "# mongo_uri = \"mongodb://localhost:27017\"\n",
    "client = MongoClient(mongo_uri)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then select the database, for example `rdm_workshop` and create a new collection named after the data file that we have loaded before. Before creating the collection we make sure that it does not exist. For this particular example, we create a `timeseries` collection, which is optimized for tabular data, where the main variable is a time. Previously, we named the column with date and time information `timestamp`, so we pass that to the `timeField` argument to make sure it is indexed properly and can be queried. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db = client[\"rdm_workshop\"]\n",
    "collection_name = file_path.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "if not collection_name in db.list_collection_names():\n",
    "\n",
    "    #Create time-series collection if it doesn't exist\n",
    "    db.create_collection(\n",
    "        collection_name,\n",
    "        timeseries={\n",
    "            \"timeField\": \"Datum\",   # Name of the main column, by which the time series is indexed \n",
    "            \"metaField\": \"metadata\",    # Name of the metadata field  \n",
    "            \"granularity\": \"seconds\"    \n",
    "        },\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, before uploading the data we need to convert it to a dictionary, which is Python's internal datatype for JSON-like data. Dictionaries in Python are **unordered**, which means that the order of columns is not preserved. Therefore we need to query data by column names and cannot rely on column ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "collection = db[collection_name]\n",
    "\n",
    "if collection.count_documents({}) == 0:\n",
    "    # Insert data into MongoDB\n",
    "    records = df.to_dict(orient=\"records\")\n",
    "    collection.insert_many(records)\n",
    "    print(\"Data uploaded successfully!\")\n",
    "else:\n",
    "    print(\"WARNING: Collection already contains data, make sure you are writing to the correct collection!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us add some metadata to help us find our data set later on. We will read the metadata from a secondary file `metadata_BZ011_Rohdaten.json`, which is structured as JSON data. The `metaField` in a timeseries has some special properties. If multiple documents share the same information in their respective `metaField`, MongoDB will only store this information once and then link it to each document. This gives us the best of both worlds between the convenience of having the data available in each document and the efficiency of storing it only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_from_file = pd.read_json(file_path_metadata)\n",
    "metadata_from_file = metadata_from_file.to_dict(orient=\"records\")\n",
    "\n",
    "print(metadata_from_file)\n",
    "\n",
    "collection.update_many(\n",
    "  {},\n",
    "  { \"$set\": { \"metadata\": metadata_from_file} }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `update_many` command above may require some additional explanation. As the name implies, it is used to update many documents in a collection at once. To do this, we first query the documents to update. By querying for an empty `{}` dictionary, the query will match all documents. The second argument is the update that we wish to apply to all selected documents. Specifically, we `set` the `metaField`, which we named `\"metadata\"` earlier, to the value `metadata_from_file`, which contains a dictionary version of the entries of the original JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this metadata in place, we can run some example queries to retrieve our data from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all documents that describe Accelerated stress test experiments\n",
    "cursor = collection.find({\"metadata.experiment_type\": \"Accelerated stress test\"})\n",
    "# Convert cursor to list of dictionaries\n",
    "data = list(cursor)\n",
    "# Convert to Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "# Print only the first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorter version\n",
    "cursor = collection.find({\"metadata.testbench\": \"BZ011\"})\n",
    "pd.DataFrame(list(cursor)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from datetime import datetime\n",
    "query_date = datetime(2024, 8, 5, 13, 30, 0)\n",
    "# Query data taken after a certain date / time\n",
    "cursor = collection.find({\n",
    "    \"Datum\": {\"$gt\": query_date}\n",
    "})\n",
    "\n",
    "pd.DataFrame(list(cursor)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can combine multiple queries to further specify which documents we want\n",
    "cursor = collection.find({\n",
    "    \"metadata.testbench\": \"BZ011\",\n",
    "    \"Datum\": {\"$gt\": query_date}\n",
    "})\n",
    "pd.DataFrame(list(cursor)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We can also just get the number of documents that match a condition\n",
    "count = collection.count_documents({\n",
    "    \"Datum\": {\"$gt\": query_date}\n",
    "})\n",
    "print(f\"Number of experiments on testbench BZ011: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
