{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research data management with Python and MongoDB\n",
    "\n",
    "This notebook details an example upload process from parsing raw data to pushing it into the MongoDB database.\n",
    "A second notebbok will follow this up with querying data and retrieving it to perform analyses on it.\n",
    "\n",
    "## Parsing files in Python\n",
    "\n",
    "Before we can push data to the database, we need to parse a datafile. EVN generously provided us with a sample dataset found in `data/BZ011_Rohdaten.dat`. Before trying top parse it we should take a look at the file ourselves to understand its structure. Doing that we should notice the following:\n",
    "- The file is structured as a csv file, but uses tabs as delimiters between values instead of commas\n",
    "- The first row of the file contains column headers as strings and all following rows contain mixed data\n",
    "- The data is a mix of strings, decimal numbers and dates\n",
    "- The decimal numbers use commas instead of decimal separators (as opposed to points, which are used in english speaking countries and therefore also in virtually all programming languages) \n",
    "- Some column headers contain special characters, i.e. `Â°C` \n",
    "\n",
    "We will take care of the last point first. Special characters often cause problems with text encoding if they are not handled consistently. Therefore we use the `chardet` module to automatically detect the encoding of our input file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "# File path\n",
    "file_path = \"./data/BZ011_Rohdaten.dat\"\n",
    "\n",
    "\n",
    "with open(file_path, \"rb\") as f:\n",
    "    result = chardet.detect(f.read(100000))  # Analyze first 100KB\n",
    "    detected_encoding = result[\"encoding\"]\n",
    "\n",
    "print(detected_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then parse the file using the function `pd.read_csv()`, which conveniently accepts arguments to adjust the decimal separator and delimiters. We also want to convert the `Datum` column into a proper date format using `pd.to_datatime()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46723/4185451134.py:3: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Datum'] = pd.to_datetime(df['Datum'])  # Change 'timestamp_column' to the actual column name\n"
     ]
    }
   ],
   "source": [
    "# Read data into pandas DataFrame\n",
    "df = pd.read_csv(file_path, delimiter=\"\\t\", encoding=detected_encoding, decimal=\",\")  # Adjust delimiter if needed# Convert timestamp column to datetime (modify 'timestamp_column' accordingly)\n",
    "df['Datum'] = pd.to_datetime(df['Datum'])  # Change 'timestamp_column' to the actual column name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we have done all the necessary parsing to get our input file into a pandas dataframe. Some additional parsing methods, like combining multiple files can be found in the Repository of the recent Python course: https://github.com/ZBT-Tools/Python_workshop , in the notebook of part 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to MongoDB\n",
    "\n",
    "To upload data to MongoDB we start by creating a connection using the `pymongo` module and particularly its `MongoClient` function. The ZBT database can be reached with the following connection string:\n",
    "`mongodb://username:password@172.16.134.8:27017/?directConnection=true&authSource=admin`, where username and password need to be replaced by your own credentials. For users that exist only on a specific database, e.g. student users, the `authSource` parameter needs to be set to that database.\n",
    "Naturally, we do not want our credentials to be plainly visible in a Python script - especially if we want to push it to a GitHub repository at some point. Any such secrets should be stored in environment variable files, which are conventionally called `.env` (but can be called however you prefer). \n",
    "\n",
    "```\n",
    "MONGODB_USER = \"username\"\n",
    "MONGODB_PASSWORD = \"password\"\n",
    "```\n",
    "Before committing your files to a git repository you should then create a `.gitignore` file, which contains the name of your environment file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "mongodb_user = os.environ.get(\"MONGODB_USER\")\n",
    "mongodb_password = os.environ.get(\"MONGODB_PASS\")\n",
    "# MongoDB connection\n",
    "mongo_uri = \"mongodb://\"+mongodb_user+\":\"+mongodb_password+\"@172.16.134.8:27017/?directConnection=true&authSource=admin\"\n",
    "# mongo_uri = \"mongodb://localhost:27017\"\n",
    "client = MongoClient(mongo_uri)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then select the database, for example `rdm_workshop` and create a new collection named after the data file that we have loaded before. Before creating the collection we make sure that it does not exist. For this particular example, we create a `timeseries` collection, which is optimized for tabular data, where the main variable is a time. Previously, we named the column with date and time information `timestamp`, so we pass that to the `timeField` argument to make sure it is indexed properly and can be queried. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db = client[\"rdm_workshop\"]\n",
    "collection_name = file_path.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "if not collection_name in db.list_collection_names():\n",
    "\n",
    "    #Create time-series collection if it doesn't exist\n",
    "    db.create_collection(\n",
    "        collection_name,\n",
    "        timeseries={\n",
    "            \"timeField\": \"Datum\",\n",
    "            \"metaField\": \"Set \",\n",
    "            \"granularity\": \"seconds\"\n",
    "        },\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, before uploading the data we need to convert it to a dictionary, which is Python's internal datatype for JSON-like data. Dictionaries in Python are **unordered**, which means that the order of columns is not preserved. Therefore we need to query data by column names and cannot rely on column ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "collection = db[collection_name]\n",
    "\n",
    "if collection.count_documents({}) == 0:\n",
    "    # Insert data into MongoDB\n",
    "    records = df.to_dict(orient=\"records\")\n",
    "    collection.insert_many(records)\n",
    "    print(\"Data uploaded successfully!\")\n",
    "else:\n",
    "    print(\"WARNING: Collection already contains data, make sure you are writing to the correct collection!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
